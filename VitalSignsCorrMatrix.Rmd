---
title: 'Classroom INC: Vital Signs Data Analysis'
author: "Robert Jackson"
date: "02/1/2018 - 03/30/2018"
output:
  word_document: default
  html_document: default
---
## The primary focus here is this: 

Can we accurately measure 21st century skill growth using Vital Signs (VS)?  Using external metrics can we find a correlation between VS measures of 21st century skills and the measures used in external survey?**

***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summaries 1 - 5
***
### Part 1: 

**Learned**

##### More students on average are excelling with to Leadership skills

The mean score for Leadership ratio (73.6%), the 3rd Quartile is 85.7%, this is the smallest gap. 12.1%

##### Relationship of mean to 3rd Quartile

  + Social - 16.2%
  + Productivity - 16.3%
  + Leadership - 12.1%
  + Initiative - 30.0%
  + Flexibility - 20%

##### Students really struggle with flexbility skills
  
There are a max of 26 total flexiblity points available.  On average students are reaching 8 (8.45) opportunties.  The average student is hitting around 4.2 of those 8.45 points.  On average students are getting 40% (39.7) of these opportunties. This is below the 64.1% average when you add the 4 other scores.  

There are a max of 18 total inititiave points available.  On average students are reaching 5 opportunities.  The average student is hitting around 3 of those 5 points.  On average students are getting 70% of those opportunities 

##### Outperforming with regards to Initiative Opportunties

Students are performing relatively high with respect to Initiative vs the other skills even though it has only 18 questions (all other categories have on average 24.5 Max question)  This may speak to the unusally large percentage of difference between the Inititiave mean and 3rd quartile.  

**Variance Analysis Summary**

Asses:  The initial data without the ratio suggest that students in VS are struggling with skills like "taking initiative" and "leadership" the most.  There seems to be some varance in the number of good decisions being made, but majority of students aren't doing to great. 

Once we were able to get a ratios of between points scored by individual students and the possible points availabe based on the number of episodes completed and which episodes in particular.

This new data shows that, unlike the initial EDA, students actually performed exceptionally well in Leadership and Initiative.  Students on average are struggling wtih flexibility, it being the only metric that the average percentage is below 50%.

## Part 3: 

### Issue with Merging Data and Losing rows

When I merge the external and the internal data using Usernames as the id variale we lose over half of the users we have in the internal dataframe.  I found out that Vital signs is coded as 3 in the survey data question 11 response.   I'm going to go back and verify how many students have a 3 for their answer.  That will give me closer around the missing data... I think. 

When I look at the original survey data over 850 students claim they have been playing vital signs in the academic year of 2017 - 18. So why does the merge only include rough 500 students at best?

There is a list of names that don't show up in the internal metrics but does appear in the external measures.

## Part 4:

### Summary of PCA

**Porition of Variance**
# PC1 is explaining 32% of the variance.
# PC2 is explaining 20% of the variance.
# PC3 is explaining 17% of the variance.

Observing PC1 we see that all the internal measures are related to both the external ones except the measure of Initiative, which has a negative relationship to the other variables. 

66% of the variance is explained in the first 3 Eigenvalues.  Its safe to say that there majority of the varience is explained by three PC.

# Part 5

### Summary of Regression

At best our models only explain 3% of the variance.  While we do have statistically significant variables (FlexR, episodes) they don't explain much.
***

```{r, echo=FALSE}
# I had to install tidyverse manually first since it doesn't show up with the "install.package" command.
library("tidyverse")
library("ggplot2")
library("reshape2")
```

```{r, echo=FALSE}
# 1 step is I'll import the data from the csv files.  These files are pulled from the MySQL database for Vital Signs.  Then we'll move on to some exploratory data analysis.  I'll later repeat this same process for the exteral survey measures.

C_Skills <- read.csv("SkillDev.VS.csv")

P_Skills <- read.csv("PointPos.VS.csv")
```



---
## Early EDA Questions
**Skill Dev for Vital Signs Data**
---
***

KEY: We'll have to first merge an earlier iteration of this data with a new one that includes the ratio of whys of points earn by in relation to if that point was available.  This will give us a more granular reflectionof the data's impact.

What type of variation occurs within my variables?

What type of covariation occurs between my variables?

---
### Variation of Skill data Initial
---
**Summary of the Intial Skill Data**
```{r, echo=FALSE}
C_Skills <- dplyr::filter(C_Skills, episodes > 0)

summary(C_Skills)
```
**Histograms of the Intial Skill Data**
```{r, echo=FALSE}
# Below we have histograms of the frequency students getting positive scores on the 5 measures of 21st century skills in VS.
ggplot(data = C_Skills) +
  geom_histogram(mapping = aes(x = Social), binwidth = 0.5) + geom_vline(xintercept =
  mean(C_Skills$Social), color = "red")
```

```{r, echo=FALSE}
ggplot(data = C_Skills) +
  geom_histogram(mapping = aes(x = Productivity), binwidth = 0.5) + geom_vline(xintercept =
  mean(C_Skills$Productivity), color="red")
```

```{r, echo=FALSE}
ggplot(data = C_Skills) +
  geom_histogram(mapping = aes(x = Leadership), binwidth = 0.5) + geom_vline(xintercept =
  mean(C_Skills$Leadership),
  color = "red")
```

```{r, echo=FALSE}
ggplot(data = C_Skills) +
  geom_histogram(mapping = aes(x = Initiative), binwidth = 0.5) + geom_vline(xintercept =
  mean(C_Skills$Initiative),
  color = "red")
```

```{r, echo=FALSE}
ggplot(data = C_Skills) +
  geom_histogram(mapping = aes(x = Flexibility), binwidth = 0.5) + geom_vline(xintercept =
  mean(C_Skills$Flexibility),
  color = "red")
```

```{r, echo=FALSE}
ggplot(data = C_Skills) +
  geom_histogram(mapping = aes(x = Total.Good.Decisions),
  binwidth = 0.5) + geom_vline(xintercept = mean(C_Skills$Total.Good.Decisions),
  color = "red")
```

```{r, echo=FALSE}
ggplot(data = C_Skills) +
  geom_histogram(mapping = aes(x = episodes), binwidth = 0.5) + geom_vline(xintercept =
  mean(C_Skills$episodes), color = "red")
```
---
**Variation with modified data**
---
# The motified data not only has the capacity to observe ratios but also has the increased score totals after more of the dialogue options were coded as decision points with 21st century skills attached.  

```{r}
C_Skills <- dplyr::rename(C_Skills, Users = User_Name)
P_Skills <- dplyr::rename(P_Skills, Users = username)
```

```{r}
CP_Skills <- merge(P_Skills, C_Skills, by = "Users")
CP_Skills <- dplyr::filter(CP_Skills, episodes > 0)
```

```{r}
CP_Skills <-
  dplyr::select(CP_Skills,
  -Social,
  -Initiative,
  -Leadership,
  -Productivity,
  -Flexibility)
```

```{r}
# Need to generate the ratio necessary accurately understand the data.  I have to make new variables based on the ratio

CP_Skills <-
dplyr::mutate(CP_Skills, SocialR = student_good_social / possible_good_social)
CP_Skills <-
dplyr::mutate(CP_Skills, ProdR = student_good_Productivity / possible_good_Productivity)
CP_Skills <-
dplyr::mutate(CP_Skills, LeadR = student_good_Leadership / possible_good_Leadership)
CP_Skills <-
dplyr::mutate(CP_Skills, InitiativeR = student_good_Initiative / possible_good_Initiative)
CP_Skills <-
dplyr::mutate(CP_Skills, FlexR = student_good_Flexibility / possible_good_Flexibility)
```

```{r}
CP_Skills2 <- CP_Skills[!is.na(CP_Skills$SocialR),]
CP_Skills2 <- CP_Skills[!is.na(CP_Skills$InitiativeR),]
sapply(CP_Skills, function(x)
sum(is.na(x)))
```

**Summary of Modified Data**
```{r}
summary(CP_Skills)
```
***
**Learned**
  
> More students on average are excelling with to Leadership skills

The mean score for Leadership ratio (73.6%), the 3rd Quartile is 85.7%, this is the smallest gap. 12.1%

> Relationship of mean to 3rd Quartile

+ Social - 16.2%
+ Productivity - 16.3%
+ Leadership - 12.1%
+ Initiative - 30.0%
+ Flexibility - 20%

> Students really struggle with flexbility skills
  
There are a max of 26 total flexiblity points available.  On average students are reaching 8 (8.45) opportunties.  The average student is hitting around 4.2 of those 8.45 points.  On average students are getting 40% (39.7) of these opportunties. This is below the 64.1% average when you add the 4 other scores.  

There are a max of 18 total inititiave points available.  On average students are reaching 5 opportunities.  The average student is hitting around 3 of those 5 points.  On average students are getting 70% of those opportunities 

> Outperforming with regards to Initiative Opportunties

Students are performing relatively high with respect to Initiative vs the other skills even though it has only 18 questions (all other categories have on average 24.5 Max question)  This may speak to the unusally large percentage of difference between the Inititiave mean and 3rd quartile.  

**Box Plot**
```{r, echo=FALSE}
D1 <- dplyr::select(CP_Skills, SocialR, ProdR, LeadR, InitiativeR, FlexR)
boxplot(D1)
```

```{r, echo=FALSE}
# Accordding to this analysis there are 0 Na values in this dataset.  After checking MySQL there are indeed no NULL values in this data. 


sum(is.na(CP_Skills$Student.ID))
sum(is.na(CP_Skills$SocialR))
sum(is.na(CP_Skills$ProductivityR))
sum(is.na(CP_Skills$LeadershipR))
sum(is.na(CP_Skills$InitiativeR))
sum(is.na(CP_Skills$FlexibilityR))
sum(is.na(CP_Skills$Total.Good.DecisionsR))
sum(is.na(CP_Skills$student_good_social))
sum(is.na(CP_Skills$possible_good_social))
sum(is.na(CP_Skills$student_good_Productivity))
sum(is.na(CP_Skills$possible_good_Productivity))
sum(is.na(CP_Skills$student_good_Leadership))
sum(is.na(CP_Skills$possible_good_Leadership))
sum(is.na(CP_Skills$student_good_Initiative))
sum(is.na(CP_Skills$possible_good_Initiative))
sum(is.na(CP_Skills$student_good_Flexibility))
sum(is.na(CP_Skills$possible_good_Flexibility))
sum(is.na(CP_Skills$Users))
sum(is.na(CP_Skills$episodes))
```

**Histogram of modified Data**
```{r, echo=FALSE}
# Below we have histograms of the frequency students getting positive scores on the 5 measures of 21st century skills in VS.

# The ratio histograms tell a very different story than the raw numbers.  Understanding the number of episodes completed by each student is important to validating whether students are actually struggling with a skill.

ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = student_good_social),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$student_good_social),
  color = "red")
  
  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = SocialR), binwidth = 0.5) + geom_vline(xintercept =
  mean(CP_Skills$SocialR), color = "red")

ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = possible_good_social),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$possible_good_social),
  color = "red")
```

```{r, echo=FALSE}

ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = student_good_Productivity),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$student_good_Productivity),
  color = "red")

  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = ProdR), binwidth = 0.5) + geom_vline(xintercept =
  mean(CP_Skills$ProdR), color = "red")
  
  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = possible_good_Productivity),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$possible_good_Productivity),
  color = "red")
```

```{r, echo=FALSE}
ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = student_good_Leadership),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$student_good_Leadership),
  color = "red")
  
  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = LeadR), binwidth = 0.5) + geom_vline(xintercept =
  mean(CP_Skills$LeadR), color = "red")


  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = possible_good_Leadership),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$possible_good_Leadership),
  color = "red")
```  

```{r,echo=FALSE}

ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = student_good_Initiative),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$student_good_Initiative),
  color = "red")

  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = InitiativeR), binwidth = 0.5) + geom_vline(xintercept =
  mean(CP_Skills$InitiativeR),
  color = "red")
  
  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = possible_good_Initiative),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$possible_good_Initiative),
  color = "red")
```

```{r,echo=FALSE}
ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = student_good_Flexibility),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$student_good_Flexibility),
  color = "red")
  
  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = FlexR), binwidth = 0.5) + geom_vline(xintercept =
  mean(CP_Skills$FlexR), color = "red")

  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = possible_good_Flexibility),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$possible_good_Flexibility),
  color = "red")
```

```{r,echo=FALSE}

ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = Total.Good.Decisions),
  binwidth = 0.5) + geom_vline(xintercept = mean(CP_Skills$Total.Good.Decisions),
  color = "red")
```

```{r,echo=FALSE}
  ggplot(data = CP_Skills) +
  geom_histogram(mapping = aes(x = episodes), binwidth = 0.5) + geom_vline(xintercept =
  mean(CP_Skills$episodes), color = "red")
```
---
## Initial
---
```{r, echo=FALSE}
# Next I want to visualize these distribution on the same plot.  I'll have to you ggplot2.
# I had to convert the data from wide to long in order to vizualize the distribution. 
D1 <- dplyr::select(C_Skills, -Student.ID, -Users)
```

```{r, echo=FALSE}
Viz <- melt(D1)
ggplot(Viz, aes(x = value, fill = variable)) +
geom_histogram(binwidth = 3) +
facet_grid(variable ~ .)
# https://stackoverflow.com/questions/26982465/r-how-can-i-plot-multiple-histograms-together
```
---
**Modified**
---
```{r, echo=FALSE}
# Next I want to visualize these distribution on the same plot.  I'll have to you ggplot2.
# I had to convert the data from wide to long in order to vizualize the distribution. 
D2 <- dplyr::select(CP_Skills, -Student.ID, -Users)
D3 <- dplyr::select(D2, FlexR, InitiativeR, ProdR, LeadR, SocialR)
```

```{r, echo=FALSE}
Viz <- melt(D3)
ggplot(Viz, aes(x=value, fill=variable)) +
  geom_histogram(binwidth=1)+
  facet_grid(variable~.)
# https://stackoverflow.com/questions/26982465/r-how-can-i-plot-multiple-histograms-together
```

## Variance Analysis Summary

Asses:  The initial data without the ratio suggest that students in VS are struggling with skills like "taking initiative" and "leadership" the most.  There seems to be some varance in the number of good decisions being made, but majority of students aren't doing to great. 

Once we were able to get a ratios of between points scored by individual students and the possible points availabe based on the number of episodes completed and which episodes in particular.

This new data shows that, unlike the initial EDA, students actually performed exceptionally well in Leadership and Initiative.  Students on average are struggling wtih flexibility, it being the only metric that the average percentage is below 50%.

We'll move on to covariance.

---
# Coveriance
---

**Correlation Plot**
*Initial*

```{r, echo= FALSE}
library("corrplot")
```

```{r, echo= FALSE}
CorSkill <- dplyr::select(C_Skills,-Student.ID,-Users)

D2 <- cor(CorSkill) # get correlations

corrplot(
D2,
type = "lower",
order = "hclust",
tl.col = "black",
method = "number"
) #plot matrix

corrplot(
D2,
type = "lower",
order = "hclust",
tl.col = "black",
method = "circle"
) #plot matrix
```

---
**Motified**
---

```{r, echo=FALSE}
CP_Skills <- dplyr::filter(CP_Skills, episodes > 0)
sapply(CP_Skills, function(x)
sum(is.na(x)))
```

```{r, echo=FALSE}
CP_Skills <- CP_Skills[!is.na(CP_Skills$SocialR),]
CP_Skills <- CP_Skills[!is.na(CP_Skills$InitiativeR),]
sapply(CP_Skills, function(x)
sum(is.na(x)))
```

```{r, echo=FALSE}
CorPoss <- dplyr::select(CP_Skills, -Student.ID, -Users)

D4 <- cor(CorPoss) # get correlations

corrplot(
D4,
type = "lower",
order = "hclust",
tl.col = "black",
method = "number"
) #plot matrix
```

```{r, echo=FALSE}
corrplot(
  D4,
  type = "lower",
  order = "hclust",
  tl.col = "black",
  method = "circle"
  ) #plot matrix
```

---
# Part Two: 
---
***

We're now going to add the survey data.  Go through the same EDA. Then finally we'll look at the correlation between the skills in VS and the measures in the survey data.

```{r, echo=FALSE}
# This data is the majority of schools and students
Survey1 <-
read.csv("SY 2017-2018 Student Pre-Program Survey_January 29, 2018_11.38.csv")
# This data is from one school/ system in Minn
Survey2 <-
read.csv("SY 2017-2018 MPS Student Pre-Program Survey_January 29, 2018_11.34.csv")
```

```{r, echo=FALSE}
Survey1a <- dplyr::select(Survey1, Q21_1:Q21_10)
Survey2a <- dplyr::select(Survey2, Q22_1:Q22_10)
```

---

**Working through Survey 1**

---

```{r, echo=FALSE}
summary(Survey1a)
```

```{r, echo=FALSE}
# no NA values

sapply(Survey1a, function(x)
sum(is.na(x)))
```

```{r, echo=FALSE}
# I need to remove the beginning part of all statements in row 2.  Then I need to change the variable name to fit the question.

Survey1a <- data.frame(lapply(Survey1a, function(x) {
gsub("Please respond to the following statements. - ", "", x)
}))

Survey2a <- data.frame(lapply(Survey2a, function(x) {
gsub("Please respond to the following statements. - ", "", x)
}))
```

```{r, echo=FALSE}
Survey1a = Survey1a[-1,]
Survey1a = Survey1a[-1,]          
```

```{r, echo=FALSE}
#I've been running into an issue converting the rows form factors into integers.  This is important for properly running any correlation.  I suspect that the high numbe of "blank" rows plays a role here.  I'll try to convert those into a character/or then remove them.

#We'll have to keep track of how many rows we'll lose before we can jsut remove data.
# Converting the results of the external survey into integars from factors so that I can use them in the correlation matrix.
Survey1b <-
data.frame(lapply(Survey1a, function(x)
as.numeric(as.character(x))))
```

```{r, echo=FALSE}
# All the blanks were converted into 0s so i haven't lost any rows of data.
which(Survey1b$Q21_1 == 67)
which(Survey1b$Q21_1 == 88)
```

```{r, echo=FALSE}
row_sub = apply(Survey1b, 1, function(row)
  all(row != 0))
  ##Subset as usual
  sapply(Survey1b, function(x)
  sum(is.na(x)))
```

```{r,echo=FALSE}
# removing the na's
Survey1b <- na.omit(Survey1b)
```

```{r, echo=FALSE}
# Note in this convertion processs we lost a little under 400 rows of data.  This is due to the Na's
A1 <- cor(Survey1b)
```

**How the Survey questions correlate**
```{r, echo=FALSE}
corrplot(
  A1,
  type = "lower",
  order = "hclust",
  tl.col = "black",
  method = "number"
  ) #plot matrix
  
  corrplot(
  A1,
  type = "lower",
  order = "hclust",
  tl.col = "black",
  method = "circle"
  ) #plot matrix
```

---

**Survey 2**

---

```{r, echo=FALSE}
sapply(Survey2a, function(x)
  sum(is.na(x)))
```

```{r, echo=FALSE}
Survey2a = Survey2a[-1,]
Survey2a = Survey2a[-1,]         
```

```{r, echo=FALSE}
# Converting the results of the external survey into integars from factors so that I can use them in the correlation matrix.
Survey2b <-
  data.frame(lapply(Survey2a, function(x)
    as.numeric(as.character(x))))
```

```{r,echo=FALSE}
# All the blanks were converted into 0s so i haven't lost any rows of data.
which(Survey2b$Q21_1 == 67)
which(Survey2b$Q21_1 == 88)
```

```{r, echo=FALSE}
row_sub = apply(Survey2b, 1, function(row)
  all(row != 0))
  ##Subset as usual
  sapply(Survey2b, function(x)
  sum(is.na(x)))
```

```{r, echo=FALSE}
# removing the na's
Survey2b <- na.omit(Survey2b)
```

```{r, echo=FALSE}
# Note in this convertion processs we lost a little under 400 rows of data.  This is due to the Na's
B1 <- cor(Survey2b)
```

**Corplot of the Minnesota Survey Questions**
```{r, echo=FALSE}
corrplot(
  B1,
  type = "lower",
  order = "hclust",
  tl.col = "black",
  method = "number"
  ) #plot matrix
  
  corrplot(
  B1,
  type = "lower",
  order = "hclust",
  tl.col = "black",
  method = "circle"
  ) #plot matrix
```

---

## Part 3: Creating the 2 cumulative measures from the External Instruments & Time to merge External and internal metrics

---
***
```{r, echo=FALSE}
# This data is the majority of schools and students
Survey1 <-
read.csv("SY 2017-2018 Student Pre-Program Survey_January 29, 2018_11.38.csv")
# This data is from one school/ system in Minn
Survey2 <-
read.csv("SY 2017-2018 MPS Student Pre-Program Survey_January 29, 2018_11.34.csv")
```

```{r}
match(3,Survey1$Q11)
match("3,",Survey1$Q11)
```

```{r, echo=FALSE}
# Cutting out the variables are not useful for our purpose regarding correlation
Survey1a <- dplyr::select(Survey1, ResponseId:Q21_10)
Survey2a <- dplyr::select(Survey2, ResponseId:Q22_10)
```

```{r, echo=FALSE}
Survey1a <- dplyr::select(Survey1a, ResponseId, Q8 , Q21_1:Q21_10)
Survey2a <- dplyr::select(Survey2a, ResponseId, Q11, Q22_1:Q22_10)
```

```{r, echo=FALSE}
Survey1a <- data.frame(lapply(Survey1a, function(x) {
  gsub("Please respond to the following statements. - ", "", x)
}))

Survey2a <- data.frame(lapply(Survey2a, function(x) {
gsub("Please respond to the following statements. - ", "", x)
}))
```

```{r, echo=FALSE}
colnames(Survey1a) <- as.character(unlist(Survey1a[1, ]))
Survey1a = Survey1a[-1,]

colnames(Survey2a) <- as.character(unlist(Survey2a[1, ]))
Survey2a = Survey2a[-1,] 
```

```{r, echo=FALSE}
Survey1a = Survey1a[-1,]
Survey2a = Survey2a[-1,] 
```

```{r,echo=FALSE}
#Merging the two data frames
SurveyM <- rbind(Survey1a, Survey2a)
```

```{r, echo=FALSE}
# Renaming the Variables for ease of use.

SurveyM2 <- dplyr::rename(
SurveyM,
"Users" = "Your Game Username",
"Q1" = "I can set an example for others.",
"Q2" = "I can contribute to group efforts.",
"Q3" = "I can take an active role in learning.",
"Q4" = "I compare possible solutions to find the best one.",
"Q5" = "I can do things on my own.",
"Q6" = "I usually have more than one source of information before making a decision.",
"Q7" = "I take steps to accomplish goals.",
"Q8" = "I can be in charge of a group of classmates.",
"Q9" = "I compare different ideas when thinking about a topic.",
"Q10" = "When faced with a decision, I realize that some choices are better than others.",
"Q11" = "I use what I know already to solve new problems.",
"Q12" = "I think about all the information I have about making new decisions."
)
```

```{r, echo=FALSE}
# Converting the survey responses to numeric so we can have a cummulative score
SurveyM2$Q1 <- as.numeric(as.character(SurveyM2$Q1))
SurveyM2$Q2 <- as.numeric(as.character(SurveyM2$Q2))
SurveyM2$Q3 <- as.numeric(as.character(SurveyM2$Q3))
SurveyM2$Q4 <- as.numeric(as.character(SurveyM2$Q4))
SurveyM2$Q5 <- as.numeric(as.character(SurveyM2$Q5))
SurveyM2$Q6 <- as.numeric(as.character(SurveyM2$Q6))
SurveyM2$Q7 <- as.numeric(as.character(SurveyM2$Q7))
SurveyM2$Q8 <- as.numeric(as.character(SurveyM2$Q8))
SurveyM2$Q9 <- as.numeric(as.character(SurveyM2$Q9))
SurveyM2$Q10 <- as.numeric(as.character(SurveyM2$Q10))
SurveyM2$Q11 <- as.numeric(as.character(SurveyM2$Q11))
SurveyM2$Q12 <- as.numeric(as.character(SurveyM2$Q12))
```

```{r, echo=FALSE}
# We're creating two variables. They effectively take into account the external measures.
SurveyM2$LCSDI <-
  (SurveyM2$Q1 + SurveyM2$Q2 + SurveyM2$Q3 + SurveyM2$Q5 + SurveyM2$Q7 + SurveyM2$Q8)

SurveyM2$CTDMPS <-
  (SurveyM2$Q4 + SurveyM2$Q6 + SurveyM2$Q9 + SurveyM2$Q10 + SurveyM2$Q11 + SurveyM2$Q12)
```

```{r, echo=FALSE}
SurveyM2 <-
  dplyr::select(SurveyM2,
  -Q1,
  -Q2,
  -Q3,
  -Q4,
  -Q5,
  -Q6,
  -Q7,
  -Q8,
  -Q9,
  -Q10,
  -Q11,
  -Q12)
```

```{r, echo=FALSE}
A1 <-
  read.csv("SkillDev.VS.17.csv") #Vital Signs Data from 08.15.2017 - Present
B1 <-
  read.csv("SY17-18.All.TestComplete.csv") #Data that has accurate User Names
```

```{r, echo=FALSE}
# Merging the other A1 data with the CP_Skills data
A1 <- dplyr::select(A1, Student.ID, User_Name)
A2 <- merge(A1, CP_Skills, by = "Student.ID")
```

```{r, echo=FALSE}
# Eliminating the variables that I don't need
B1 <- dplyr::select(B1, Student, STAR.RTL.ID, Student.Survey.Match)
```

```{r, echo=FALSE}
#Need to merge the survey response data from SurveyM2 with the accurate User Name & ID data from B1
B2 <- dplyr::rename(B1, Response_ID = Student.Survey.Match)
SurveyM3 <- dplyr::rename(SurveyM2, Response_ID = "Response ID")
B3 <- merge(B2, SurveyM3, by = "Response_ID")
```

```{r, echo=FALSE}
# merge of the the Survey Data w/ the two measures and their accurate Usernames
summary(B3)
```

```{r, echo=FALSE}
library(psych)
describe(B3)
```

```{r, echo=FALSE}
T1 <- dplyr::select(B3, STAR.RTL.ID)
T2 <- dplyr::select(B3, Users)

identical(T1, T2) # the user names don't match
```
```{r, echo=FALSE}
# Testing which user name holds on to more users when merged with the internal measures. 
T1 <- dplyr::select(B3, - Users)
T2 <- dplyr::select(B3, -STAR.RTL.ID )
A3 <-
dplyr::select(A2,
Users,
Total.Good.Decisions,
episodes,
SocialR,
ProdR,
LeadR,
InitiativeR,
FlexR)
A3 <-dplyr::rename(A3, User1 = Users)
T1 <- dplyr::rename(T1, User1 = STAR.RTL.ID)
Test1 <- merge(T1, A3, by = "User1")
```

```{r,echo=FALSE}
T2 <- dplyr::select(B3, -STAR.RTL.ID )
T2 <- dplyr::rename(T2, User1 = Users)
Test2 <- merge(T2, A3, by = "User1")
```

```{r}
write.csv(M1, file = "DATAVS.csv")
```

## Issue with Merging Data and Losing rows

When I merge the external and the internal data using Usernames as the id variale we lose over half of the users we have in the internal dataframe.  I found out that Vital signs is coded as 3 in the survey data question 11 response.   I'm going to go back and verify how many students have a 3 for their answer.  That will give me closer around the missing data... I think. 

When I look at the original survey data over 850 students claim they have been playing vital signs in the academic year of 2017 - 18. So why does the merge only include rough 500 students at best?

There is a list of names that don't show up in the internal metrics but does appear in the external measures.

```{r, echo = FALSE}
# Need to merge the data we used TEST1
A3 <- dplyr::select(A2, -User_Name)
A3 <-
dplyr::select(A3,
Users,
Total.Good.Decisions,
episodes,
SocialR,
ProdR,
LeadR,
InitiativeR,
FlexR)
B3 <- dplyr::select(B3, -Users)
B3 <- dplyr::rename(B3, Users = STAR.RTL.ID)
M1 <- merge(A3, B3, by = "Users")
```

```{r, echo=FALSE}
summary(M1)
```

```{r, echo=FALSE}
library(corrplot)
```

```{r, echo=FALSE}
# Converting the results of the external survey into integars from factors so that I can use them in the correlation matrix.
M2 <- dplyr::select(M1,-Response_ID,-Student,-Users)

M3 <- na.omit(M2)

MCor <- cor(M3)
```

## Correlation plot of the External measures and the Internal measures from VS
```{r, echo=FALSE}
corrplot(MCor,type = "lower", order = "hclust", tl.col = "black", method = "number") #plot matrix
```

```{r, echo=FALSE}
corrplot(MCor,type = "lower", order = "hclust", tl.col = "black", method = "circle") #plot matrix
```

---
## Part 4: PCA/ Factor Analysis
---
***

```{r, echo=FALSE}
#principal component analysis
prin_comp <- prcomp(M3, scale. = T)
names(prin_comp)
```

```{r, echo=FALSE}
prin_comp$rotation
biplot(prin_comp, scale = 0)
summary(prin_comp)
```

```{r, echo=FALSE}
std_dev <- prin_comp$sdev

#compute variance
pr_var <- std_dev ^ 2
```


```{r,echo=FALSE}
#proportion of variance explained
prop_varex <- pr_var / sum(pr_var)
prop_varex[1:20]
```

```{r, echo=FALSE}
#scree plot
plot(prop_varex,
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b")
```


```{r,echo=FALSE}
#cumulative scree plot
plot(cumsum(prop_varex),
     xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")
     
```

## Summary of PCA

**Porition of Variance**
# PC1 is explaining 32% of the variance.
# PC2 is explaining 20% of the variance.
# PC3 is explaining 17% of the variance.

Observing PC1 we see that all the internal measures are related to both the external ones except the measure of Initiative, which has a negative relationship to the other variables. 

66% of the variance is explained in the first 3 Eigenvalues.  Its safe to say that there majority of the varience is explained by three PC.

**Rerunning the PCA with a different package**
```{r,echo=FALSE}
library("ggplot2")
library("FactoMineR")
library("factoextra")
```

```{r,echo=FALSE}
# The R code below, computes principal component analysis on the active individuals/variables:

M3.pca <- PCA(M3, graph = FALSE)
print(M3.pca)
```
```{r,echo=FALSE}
eig.val <- get_eigenvalue(M3.pca)
eig.val
```
```{r,echo=FALSE}
fviz_eig(M3.pca, addlabels = TRUE, ylim = c(0, 50))
```

```{r, echo=FALSE}
var <- get_pca_var(M3.pca)
var
```

```{r, echo=FALSE}
# Coordinates of variables
head(var$coord, 9)
```

```{r, echo=FALSE}
fviz_pca_var(M3.pca, col.var = "black")
```

```{r, echo=FALSE}
corrplot(var$cos2, is.corr=FALSE)
```

## How the variables cluster together
```{r, echo=FALSE}
# Create a grouping variable using kmeans
# Create 3 groups of variables (centers = 3)
set.seed(123)
res.km <- kmeans(var$coord, centers = 3, nstart = 25)
grp <- as.factor(res.km$cluster)
# Color variables by groups
fviz_pca_var(
M3.pca,
col.var = grp,
palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
legend.title = "Cluster"
)
```

```{r, echo=FALSE}
M3.desc <- dimdesc(M3.pca, axes = c(1, 2), proba = 0.05)
# Description of dimension 1
M3.desc$Dim.1
```

```{r, echo=FALSE}
# Description of dimension 2
M3.desc$Dim.2
```

```{r, echo=FALSE}
# Description of dimension 3
M3.desc$Dim.3
```
## Part 5: Regression
**Regression LCSDI**

_the varibles that make up majority of PC1 as it relates to LCSDI (External1)_
I will run a regression on the variables taht have over 75% correlation to PC1
```{r, echo=FALSE}
External1.a <-
  lm(LCSDI ~  +FlexR + Total.Good.Decisions + episodes, data = M3)
  summary(External1.a)
```

```{r, echo=FALSE}
External1.b <-
  lm(LCSDI ~  SocialR + FlexR + Total.Good.Decisions + episodes, data = M3)
  summary(External1.b)
```

_the varibles that make up majority of PC1 as it relates to CTDMPS (External2)_
I will run a regression on the variables taht have over 75% correlation to PC1
```{r, echo=FALSE}
External2.a <-
  lm(CTDMPS ~  +FlexR + Total.Good.Decisions + episodes, data = M3)
  summary(External2.a)
```

```{r, echo=FALSE}
External2.b <-
  lm(CTDMPS ~ SocialR + FlexR + Total.Good.Decisions + episodes, data = M3)
  summary(External2.b)
```
### Summary
At best our models only explain 3% of the variance.  While we do have statistically significant variables (FlexR, episodes) they don't explain much.

 
```{r}

```
 
 
 